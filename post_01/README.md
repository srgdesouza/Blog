# Attention, a Very Simple Explanation

Welcome to the companion repository for the blog post **"Attention, a Very Simple Explanation"** by Sergio. This post breaks down the core ideas behind attention mechanisms in machine learning â€” especially in transformer models â€” using intuitive examples and clear language.

## ðŸ“˜ Blog Overview

The blog explores:
- What attention mechanisms are and why they matter
- How models learn to focus on relevant words in a sentence
- The difference between bidirectional and causal attention
- Real-world examples using ambiguous words like "lead" and "bank"
- Visual illustrations to make the concepts easy to grasp

## ðŸ”— Colab Notebook

To complement the blog, weâ€™ve created a Google Colab notebook that implements the key concepts discussed, including:
- Token-level attention visualization
- Causal masking in generative models
- Simple transformer-style attention calculations

ðŸ‘‰ [Open the Colab Notebook](https://github.com/srgdesouza/Blog/blob/89861c76c7b1e68c4d11fef93c605c159cb4f21f/post_01/Blog_Self_Attention_Basic.ipynb) *(replace with actual link)*

##  Who Is This For?

This resource is ideal for:
- Beginners curious about how transformers work
- Educators looking for intuitive ways to teach attention
- Anyone who wants a jargon-free introduction to one of the most important ideas in modern AI

## ðŸ“¬ Feedback

Feel free to open an issue or reach out if you have suggestions or questions!

---

Made by Sergio
